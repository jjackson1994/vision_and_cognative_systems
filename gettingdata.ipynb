{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b6acebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already existed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# set where you want to store the data here, we will make it for you. Do not include a last slash.\n",
    "data_folder = '/home/ubuntu/brackish_data'\n",
    "#if the data folder does not exist then make the folder\n",
    "if not os.path.exists(data_folder):\n",
    "    !mkdir $data_folder\n",
    "    print('We made the folder for you:',data_folder)\n",
    "else: \n",
    "    print('The folder already existed')\n",
    "\n",
    "# base = '/home/ubuntu/Brackish/'\n",
    "# annot = 'annotations/annotations_AAU/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8a7df",
   "metadata": {},
   "source": [
    "#### Download Data\n",
    "Kaggle download links do not work with wget. There is also no good api for downlopading through the commandline. So download the brackish dataset and place it in *data_folder*. You should have a file called *archive.zip*. You can upload it with the jupyter notebook client. \n",
    "\n",
    "https://www.kaggle.com/datasets/aalborguniversity/brackish-dataset\n",
    "\n",
    "#### Unzip Data\n",
    "Once downloaded, the next cell will unzip it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5b85eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder [archive] exists, so we think archive.zip is already unzipped\n"
     ]
    }
   ],
   "source": [
    "#unzipping the file\n",
    "unzipped = os.path.exists(data_folder+'/archive')\n",
    "if not unzipped:\n",
    "    !unzip -q $data_folder/archive.zip -d $data_folder/archive\n",
    "    print('we have unzipped it for you')\n",
    "else: print('The folder [archive] exists, so we think archive.zip is already unzipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda4b94",
   "metadata": {},
   "source": [
    "#### Extracting images from videos\n",
    "The creators of the kaggle dataset have provided a script for extracting the images from the video files. It assigns each image the correct file name that is used in the provided annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5269d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the folder to system directory so we can access their script\n",
    "# You may need to install some dependancies\n",
    "#ffmpeg, conda install -c conda-forge ffmpeg\n",
    "#ipdb, conda install -c conda-forge ipdb\n",
    "import sys \n",
    "# adding Folder_2 to the system path\n",
    "sys.path.insert(0, data_folder+'/archive/scripts')\n",
    "from frameExtractor import extractFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "72b119d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: [-d: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# !rm -R $data_folder/images\n",
    "![-d \"$data_folder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5f193af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder /home/ubuntu/brackish_data/images already exists. So we assumed the images are already extracted.\n"
     ]
    }
   ],
   "source": [
    "frames_extracted = os.path.exists(data_folder+'/images')\n",
    "folders = ['crab','fish-big','fish-school','fish-small-shrimp','jellyfish']\n",
    "if not frames_extracted:\n",
    "    for fol in folders:\n",
    "        extractFrames({'inputFolder':data_folder+'/archive/dataset/videos/'+fol, 'outputFolder':data_folder+'/images'});\n",
    "    print('We have extracted all the frames for you and placed them in:', data_folder+'/images')\n",
    "else:\n",
    "    print('The folder',data_folder+'/images','already exists. So we assumed the images are already extracted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d940bc",
   "metadata": {},
   "source": [
    "#### Make final data folder\n",
    "To make things simple we copy the necessary files for pytorch to one folder and leave the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d2fb2aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ubuntu/brackishData’: File exists\n",
      "cp: cannot stat '/home/ubuntu/brackish_data/annotations/annotations_AUU': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/brackishData\n",
    "!cp -R $data_folder/images ~/brackishData\n",
    "!cp -R $data_folder/annotations/annotations_AUU ~/brackishData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4bcfc",
   "metadata": {},
   "source": [
    "resources used to make the dataset  \n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html  \n",
    "https://www.youtube.com/watch?v=ZoZHd0Zm3RY  \n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html   \n",
    "https://github.com/pytorch/vision.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa182833",
   "metadata": {},
   "source": [
    "#### Making the class\n",
    "The standard way to load data into pytorch is with a custom data class. I used the following resources to construct this class.  \n",
    "  \n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html  \n",
    "https://www.youtube.com/watch?v=ZoZHd0Zm3RY   \n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html  \n",
    "https://github.com/pytorch/vision.git   \n",
    "\n",
    "This will be in a brackishData.py file that will be in the git repo to be imported into your notebooks.\n",
    "\n",
    "Some examples for how to use the class are given after it is made.\n",
    "\n",
    "This in theory could be used in the place of standard pytorch data sets you will find in tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb8296bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "\n",
    "class brackish_data(Dataset):\n",
    "    label_key = {'fish':0,'small_fish':1,'crab':2,'shrimp':3,'jellyfish':4,'starfish':5}\n",
    "    def __init__(self, annotations_file, images_folder, transform=None):\n",
    "        cnames = ['filename', 'object_id', 'label', 'up_left_x','up_left_y','low_right_x','low_right_y']\n",
    "        self.annotations = pd.read_csv(annotations_file, delimiter=';',skiprows=1,names=cnames)\n",
    "        \n",
    "        self.filenames = self.annotations['filename'].drop_duplicates()\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        ulx, uly, lrx, lry = self.annotations['up_left_x'], self.annotations['up_left_y'], self.annotations['low_right_x'], self.annotations['low_right_y']\n",
    "        self.annotations['area'] = (lrx - ulx) * (lry - uly)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        filename = self.filenames.iloc[index]\n",
    "        img_path = os.path.join(self.images_folder,filename)\n",
    "        image = io.imread(img_path)\n",
    "        \n",
    "        annotations_filtered = self.annotations[self.annotations['filename']==filename]\n",
    "        \n",
    "        boxes = annotations_filtered[['up_left_x','up_left_y','low_right_x','low_right_y']]\n",
    "        #might need to add torch.cuda later when cuda is enabled\n",
    "        boxes = torch.FloatTensor(boxes.values.tolist())\n",
    "        \n",
    "        labels = annotations_filtered['label'].tolist()\n",
    "        labels_int = [label_key[lbl] for lbl in labels]\n",
    "        #might need to add torch.cuda later when cuda is enabled\n",
    "        labels_int = torch.LongTensor(labels_int)\n",
    "        \n",
    "        area = torch.tensor(annotations_filtered['area'].tolist())\n",
    "        \n",
    "        target = {'boxes':boxes,'labels':labels_int,'image_id':filename,'area':area,'iscrowd':False}\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "data_folder = '/home/ubuntu/brackishData'\n",
    "train_data = brackish_data(annotations_file=data_folder+'/annotations_AUU/train.csv', images_folder=data_folder+'images/',transform = torchvision.transforms.ToTensor())\n",
    "test_data = brackish_data(annotations_file=data_folder+'/annotations_AUU/test.csv', images_folder=data_folder+'images/',transform = torchvision.transforms.ToTensor())\n",
    "valid_data = brackish_data(annotations_file=data_folder+'/annotations_AUU/valid.csv', images_folder=data_folder+'images/',transform = torchvision.transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e2ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch2]",
   "language": "python",
   "name": "conda-env-pytorch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
