{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KzQAOWJq7EN"
      },
      "source": [
        "# NEURAL NETWORKS AND DEEP LEARNING\n",
        "\n",
        "---\n",
        "A.A. 2022/23 (6 CFU) - Dr. Jacopo Pegoraro, Dr. Daniele Mari\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAB1YHM4lHK5"
      },
      "source": [
        "# Residual and Inception Neural Networks\n",
        "\n",
        "Welcome to the sixth HDA laboratory! In this notebook, you will implement two advanced architecutres. \n",
        "\n",
        "1. **Residual neural network (ResNet).** ResNet have been introduced by [He et al.](https://arxiv.org/pdf/1512.03385.pdf) to effectively train deep neural networks. In fact, deep neural networks can represent very complex functions but in practice, are hard to train. \n",
        "2. **Inception-v4 network.** This architecture was proposed by [Google developers](https://arxiv.org/pdf/1602.07261.pdf) for image classification.\n",
        "\n",
        "**In this assignment, you will:**\n",
        "- Implement the basic building blocks of ResNets and Inception-v4. \n",
        "- Put together these building blocks to implement and train a state-of-the-art neural network for image classification. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF5GeovQP1nA"
      },
      "source": [
        "The dataset is shared here https://drive.google.com/drive/folders/1fAnEoHWYetbyO5-grMmvabGjLUMgrS-Q?usp=share_link. Add a shortcut to your own google drive and mount drive on google colab. this way you should be able to access the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9jI_IscxN4WZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04bd0cb-a41e-4542-f58c-1a2a14e9e6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi1AoyaQm8Pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976321fd-b37f-48b9-b7dc-eaceabf48a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "! cp /content/drive/MyDrive/dataset_nndl/camelyon* ./\n",
        "! mv camelyonpatch_level_2_split_test_y.h5.gz.gzip  camelyonpatch_level_2_split_test_y.h5.gz\n",
        "! mv camelyonpatch_level_2_split_valid_y.h5.gz.gzip  camelyonpatch_level_2_split_valid_y.h5.gz\n",
        "! mv camelyonpatch_level_2_split_train_y.h5.gz.gzip  camelyonpatch_level_2_split_train_y.h5.gz\n",
        "! gzip -df camelyonpatch_level_2_split_test_x.h5.gz \n",
        "! gzip -df camelyonpatch_level_2_split_test_y.h5.gz \n",
        "! gzip -df camelyonpatch_level_2_split_valid_x.h5.gz \n",
        "! gzip -df camelyonpatch_level_2_split_valid_y.h5.gz\n",
        "! gzip -df camelyonpatch_level_2_split_train_x.h5.gz \n",
        "! gzip -df camelyonpatch_level_2_split_train_y.h5.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k6XGNMSlMCs"
      },
      "source": [
        "# Dataset\n",
        "For this lab you will use the [**PatchCamelyon** dataset](https://github.com/basveeling/pcam). It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue.\n",
        "A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. Tumor tissue in the outer region of the patch does not influence the label. \n",
        "\n",
        "In this notebook you will use a smaller version of the dataset that consists of 32.000 images. Feel free to change the __len__method to experiment with a biggere dataset (e.g., if you train the network with more examples, the performance of the designed classifier should increase)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Fgr639p6u8"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = Compose([\n",
        "    ToTensor(), #this converts numpy or Pil image to torch tensor and normalizes it in 0, 1\n",
        "    RandomAffine((0.05, 0.05)),\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomVerticalFlip()\n",
        "])"
      ],
      "metadata": {
        "id": "cELhUwgD40L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfbpEc7VsAgg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_folder, dataset_type, transform=None):\n",
        "        self.x = h5py.File(os.path.join(dataset_folder, f'camelyonpatch_level_2_split_{dataset_type}_x.h5'), 'r')['x']\n",
        "        self.y = h5py.File(os.path.join(dataset_folder, f'camelyonpatch_level_2_split_{dataset_type}_y.h5'), 'r')['y']\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return min(32000, len(self.x))\n",
        "        #return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        y = self.y[idx, 0, 0].astype(float)\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoyZe382xFzQ"
      },
      "outputs": [],
      "source": [
        "train_dataset =  ImageDataset(\"./\", \"train\", transforms)\n",
        "valid_dataset =  ImageDataset(\"./\", \"valid\", ToTensor())\n",
        "test_dataset =  ImageDataset(\"./\", \"test\", ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSDkCKuDYfDd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(4, 4, figsize=(6,6))\n",
        "for i in range(16):\n",
        "    axs[i//4][i%4].imshow(train_dataset[i][0].permute(1, 2, 0).numpy())\n",
        "    axs[i//4][i%4].set_xticks([])\n",
        "    axs[i//4][i%4].set_yticks([])\n",
        "    axs[i//4][i%4].set_title(f\"class: {train_dataset[i][1]}\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfP9b44Dx2Zc"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count())\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
        "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQFY2_Eiykoo"
      },
      "source": [
        "# Residual Networks\n",
        "In recent years, neural networks have become deeper, with state-of-the-art networks going from just a few layers to over a hundred layers.\n",
        "\n",
        "* The main benefits of deep networks are that they can represent very complex functions and learn features at different levels of abstraction, from edges (at the shallower layers, closer to the input) to very complex features (at the deeper layers, closer to the output). \n",
        "* However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent slow. \n",
        "* More specifically, during gradient descent, as you backpropagate from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and \"explode\" to take very large values). \n",
        "\n",
        "Residual networks try to solve this problem by using skip connections as explained in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akm6HMcFyo8o"
      },
      "source": [
        "## 1 - Building blocks and construction\n",
        "\n",
        "In ResNets, a \"shortcut\" or a \"skip connection\" allows the model to skip layers:  \n",
        "\n",
        "<img src=\"https://theaisummer.com/static/8d19d048cd68d6dce362e025cf3b635a/1ac66/skip-connection.png\" scale=\"50%\">\n",
        "<caption><center> A ResNet block showing a skip-connection <br> </center></caption>\n",
        "\n",
        "The image on the left shows the \"main path\" through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network. \n",
        "\n",
        "Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. You are going to implement both of them: the \"identity block\" and the \"convolutional block.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdwHEDdRyr8D"
      },
      "source": [
        "### 1.1 - The identity block\n",
        "\n",
        "The identity block is the standard block used in ResNets, and corresponds to the case where the input activation ($a^{[l]}$) has the same dimension as the output activation ($a^{[l+2]}$).\n",
        "In this exercise, you will implement an identity block in which the skip connection \"skips over\" 3 hidden layers:\n",
        "\n",
        "<img src=\"https://datascience-enthusiast.com/figures/idblock3_kiank.png\" width=\"1000px\">\n",
        "<caption><center> Identity block. Skip connection \"skips over\" 2 layers. </center></caption>\n",
        "\n",
        "The upper path is the \"shortcut path\". The lower path is the \"main path\" with convolutional layers and ReLu activation functions. As introduced in the other laboratories, to speed up training we also add BatchNorm layers.  \n",
        "\n",
        "Here are the individual steps.\n",
        "\n",
        "**First component** of main path: \n",
        "- CONV2D with $F_1$ filters of shape (1, 1), stride of (1, 1), padding \"valid\".\n",
        "- BatchNorm, normalizing the 'channels' axis.\n",
        "- ReLU activation function. \n",
        "\n",
        "**Second component** of main path:\n",
        "- CONV2D with $F_2$ filters of shape $(f, f)$, stride of (1, 1), padding \"same\". \n",
        "- BatchNorm, normalizing the 'channels' axis.\n",
        "- ReLU activation function. \n",
        "\n",
        "**Third component** of main path:\n",
        "- CONV2D with $F_3$ filters of shape (1, 1), stride of (1, 1), padding \"valid\". \n",
        "- BatchNorm, normalizing the 'channels' axis. \n",
        "- Note that there is **no** ReLU activation function in this component. \n",
        "\n",
        "**Final step**: \n",
        "- Add together the `X_input` and the output from the 3rd layer `X` (shortcut).\n",
        "- ReLU activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6EOUZkGylPS"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU\n",
        "\n",
        "class MainPath(Module):\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size, stride=1):\n",
        "        super().__init__()\n",
        "        F1, F2, F3 = filters\n",
        "        self.main_path = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.main_path(x)\n",
        "        return y\n",
        "\n",
        "class IdentityBlock(MainPath):\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size):\n",
        "        super().__init__(in_channels, filters, kernel_size)\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = None\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_block = IdentityBlock(3, [100, 100, 3], 3)\n",
        "print(id_block(torch.zeros(1, 3, 10, 10)).shape)\n",
        "print(\"expected shape: (1, 3, 10, 10)\")"
      ],
      "metadata": {
        "id": "cLB8ziT-Tuhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWcavxcl2gPF"
      },
      "source": [
        "### 1.2 - The convolutional block\n",
        "\n",
        "The ResNet \"convolutional block\" is the second block type. You can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path: \n",
        "\n",
        "<img src=\"https://datascience-enthusiast.com/figures/convblock_kiank.png\">\n",
        "\n",
        "<caption><center> Convolutional block </center></caption>\n",
        "\n",
        "* The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path.  \n",
        "* For example, to reduce the activation dimensions's height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. \n",
        "* The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. \n",
        "\n",
        "The details of the convolutional block are as follows. \n",
        "\n",
        "**First component** of main path:\n",
        "- CONV2D with $F_1$ filters of shape (1, 1), stride of (s, s), padding \"valid\".\n",
        "- BatchNorm. \n",
        "- ReLU activation function.\n",
        "\n",
        "**Second component** of main path:\n",
        "- CONV2D with $F_2$ filters of shape (f, f), stride of (1, 1), padding \"same\".\n",
        "- BatchNorm. \n",
        "- ReLU activation function. \n",
        "\n",
        "**Third component** of main path:\n",
        "- CONV2D with $F_3$ filters of shape (1, 1), stride of (1,1), padding \"valid\".\n",
        "- BatchNorm. \n",
        "Note that there is no ReLU activation function in this component. \n",
        "\n",
        "**Shortcut path**:\n",
        "- CONV2D with $F_3$ filters of shape (1, 1), stride of (s, s), padding \"valid\".\n",
        "- BatchNorm. \n",
        "\n",
        "**Final step**: \n",
        "- Add together the shortcut and the main path values.\n",
        "- ReLU activation function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owub4yLx2fWs"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalBlock(MainPath):\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size):\n",
        "        super().__init__(in_channels, filters, kernel_size, stride=2)\n",
        "        self.relu = ReLU()\n",
        "        self.shortcut_path = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = None\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_block = ConvolutionalBlock(3, [100, 100, 3], 3)\n",
        "print(id_block(torch.zeros(1, 3, 10, 10)).shape)\n",
        "print(\"expected shape: (1, 3, 5, 5)\")"
      ],
      "metadata": {
        "id": "1CBEDnthZskC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCw1QIwu7_3v"
      },
      "source": [
        "### 1.3 - Building the ResNet model (50 layers)\n",
        "\n",
        "You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x n\" means you should stack n identity blocks together.\n",
        "\n",
        "The details of this ResNet-50 model are:\n",
        "- Stage 1:\n",
        "    - 2D Convolution with 64 filters of shape (7, 7) and stride of (2, 2).\n",
        "    - BatchNorm, applied to the 'channels' axis of the input.\n",
        "    - MaxPooling with (3, 3) window and (2, 2) stride.\n",
        "- Stage 2:\n",
        "    - Convolutional block with three sets of filters of size [64, 64, 256], `f = 3` and `s = 1`.\n",
        "    - 2 identity blocks with three sets of filters of size [64,64,256], `f = 3`.\n",
        "- Stage 3:\n",
        "    - Convolutional block with three sets of filters of size [128, 128, 512], `f = 3` and `s = 2`.\n",
        "    - 3 identity blocks with three sets of filters of size [128, 128, 512], `f = 3`.\n",
        "- Stage 4:\n",
        "    - Convolutional block with three sets of filters of size [256, 256, 1024], `f = 3` and `s = 2`.\n",
        "    - 5 identity blocks with three sets of filters of size [256, 256, 1024], `f = 3`.\n",
        "- Stage 5:\n",
        "    - Convolutional block with three sets of filters of size [512, 512, 2048], `f = 3` and `s = 2`.\n",
        "    - 2 identity blocks with three sets of filters of size [512, 512, 2048], `f = 3`.\n",
        "- 2D Average Pooling with window of shape (2, 2).\n",
        "- Flatten layer.\n",
        "- Fully Connected (Dense) layer reduces its input to one single neuron at output using sigmoid activation.\n",
        "\n",
        "Implement below the ResNet50 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EznUa7NN7_dp"
      },
      "outputs": [],
      "source": [
        "from torch.nn import MaxPool2d, AvgPool2d, Linear, Dropout\n",
        "class ResNet50(Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.classification_layer = None\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = None\n",
        "        y = None\n",
        "        return y\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpz1QhHg1rRu"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from tqdm import tqdm\n",
        "model = ResNet50()\n",
        "opt = SGD(model.parameters(), lr=1e-2, weight_decay = 0)\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "epochs=10\n",
        "best_val = np.inf\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    iterator = tqdm(train_dataloader)\n",
        "    for batch_x, batch_y in iterator:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        y_pred = model(batch_x)\n",
        "\n",
        "        loss = loss_fn(y_pred, batch_y)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        true = []\n",
        "        for batch_x, batch_y in tqdm(valid_dataloader):\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            y_pred = model(batch_x)\n",
        "\n",
        "            predictions.append(y_pred)\n",
        "            true.append(batch_y)\n",
        "        predictions = torch.cat(predictions, axis=0)\n",
        "        true = torch.cat(true, axis=0)\n",
        "        val_loss = loss_fn(predictions, true)\n",
        "        val_acc = (torch.sigmoid(predictions).round() == true).float().mean()\n",
        "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
        "    \n",
        "    if val_loss < best_val:\n",
        "        print(\"Saved Model\")\n",
        "        torch.save(model.state_dict(), \"model.pt\")\n",
        "        best_val = val_loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqlKcMzg-MeQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_curve, roc_auc_score\n",
        "\n",
        "def evaluate_network(dataloader, model, data_split):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        true = []\n",
        "        for batch_x, batch_y in tqdm(dataloader):\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            y_pred = model(batch_x)\n",
        "\n",
        "            predictions.append(y_pred)\n",
        "            true.append(batch_y)\n",
        "        predictions = torch.cat(predictions, axis=0)\n",
        "        true = torch.cat(true, axis=0)\n",
        "        loss = loss_fn(predictions, true).detach().cpu().numpy()\n",
        "        predictions = torch.sigmoid(predictions).detach().cpu().numpy()\n",
        "        true = true.detach().cpu().numpy()\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(true, predictions)\n",
        "        auc = roc_auc_score(true, predictions)\n",
        "        predictions = predictions.round()\n",
        "        precision, recall, fscore, _= precision_recall_fscore_support(true, predictions, average='binary')\n",
        "        accuracy = accuracy_score(true, predictions)\n",
        "\n",
        "        print(f\"{data_split} loss: {loss}, accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1: {fscore}, roc_auc: {auc}\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{data_split} receiver operating characteristic (ROC)')\n",
        "        plt.legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3BsW-HIafCF"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "evaluate_network(train_dataloader, model, \"Training Dataset\")\n",
        "evaluate_network(valid_dataloader, model, \"Validation Dataset\")\n",
        "evaluate_network(test_dataloader, model, \"Test Dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPSLs-U4KT4E"
      },
      "source": [
        "# Inception network\n",
        "\n",
        "In a pure Inception network there are two different block types: the *Inception blocks* and the *Reduction blocks*.\n",
        "\n",
        "Inception-v4 is composed of three different Inception blocks, two Reduction ones and an initial stem block. \n",
        "\n",
        "<img src=\"https://developer.ridgerun.com/wiki/images/thumb/2/29/Inceptionv4.png/720px-Inceptionv4.png\" width=\"500px\">\n",
        "<caption><center>  <br> </center></caption>\n",
        "\n",
        "### Rationale behind Inception layers\n",
        "\n",
        "Inception layers are usually built by runnining in parallel multiple different branches. As an example see the Inception-A block.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1wth4q2nf9h4d4JTwmrAhLVzb5pFyIRCd\" width=\"600px\">\n",
        "<caption><center> Inception-A block. </center></caption>\n",
        "\n",
        "The main improvements introduced by the inception network are the following:\n",
        "\n",
        "* Since the various branches of a layer have filters of different sizes, they will focus on features with different scales. This is usually obtained by either increasing the kernel size of the filters or by stacking multiple Conv layers one after the other. For example by stacking two 3x3 layers the receptive field at the output is going to be 5x5. This considerably increases the feature extraction capability of the model.\n",
        "\n",
        "* The 1x1 convolutional layers introduced before each branch are used to reduce considerably the total number of operations in the network (FLOPS). Consider the fact that a conv layer with kernel size NxN and F filters that processes an input of size HxWxC computes approximately N\\*N\\*F\\*H\\*W\\*C operations. Given this it is possible to see that in general if we simply apply a NxN convolutional layer we get in total N\\*N\\*F\\*H\\*W\\*C operations while by adding a 1x1 layer first that has a number of output filters F1 < C then we get a total number of operations equal to H\\*W\\*(F1\\*C + N\\*N\\*F2\\*F1) operations. It is easy to see that this is in general less FLOPs than when running a single NxN layer if F1 is small enough. Of course the bigger N the better the gain in terms of computational complexity. What the 1x1 convolutional filter actually does is that it performs dimensionality reduction since it is simply reducing the number of channels in the input representation while retaining as much information as possible.\n",
        "For example in the Inception-A block the 1x1 convolution + 3x3 convolution reduces the total number of operations from 3\\*3\\*96\\*H\\*W\\*384 = 331776\\*H\\*W\n",
        "to H\\*W(384\\*64 + 3\\*3\\*64\\*96) = 79872 \\*H\\*W  which is a 4x improvement and the gap increases considerably when we increase the filter size (e.g. 7x7)\n",
        "\n",
        "* The authors have shown that expecially for big filters it is possible to reduce the overall number of FLOPs considerably at the price of very low performance loss by substituting NxN convolutional layers with two layers one with filters of size Nx1, and one with filters of size Nx1. The resulting receptive field is the same and the performance does not change much, however the total number of operations is 2\\*N\\*F\\*H\\*W\\*C instead of N\\*N\\*F\\*H\\*W\\*C. Additionally if the first layer also reduces the number of channels of the input then extra operations are saved.\n",
        "\n",
        "* For the same reason also stacking multiple 3x3 layers is usually better than using a 5x5 or 7x7 layer in terms of number of operations while maintaining the same receptive field.\n",
        "\n",
        "All of these tricks allow defining a very powerful model but that is much more efficient w.r.t. the naive definition that simply stacks NxN convolutions.\n",
        "\n",
        "\n",
        "**Note**: binary classification task.\n",
        "\n",
        "## 1 - Inception-v4 Building blocks \n",
        "### 1.1 - Convolutional and batch normalization helper function\n",
        "First of all, here below is implemented the ``Conv2d_bn`` helper module that you will use in all the blocks of the Inception v4 network.\n",
        "\n",
        "Use the following structure:\n",
        "- CONV2D with $F$ filters of shape ($h$, $w$), stride of ($s_1$, $s_2$). \n",
        "- BatchNorm, normalizing the 'channels' axis.  \n",
        "- ReLU activation function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgQ7W0qKr2Do"
      },
      "outputs": [],
      "source": [
        "class Conv2d_bn(Module):\n",
        "\n",
        "    def __init__(self, in_filters, out_filters, kernel_size, strides, padding):\n",
        "        super().__init__()\n",
        "        if isinstance(kernel_size, tuple):\n",
        "            padding_val = (k // 2 for k in kernel_size) if padding == \"same\" else (0,0)\n",
        "        else:\n",
        "            padding_val = kernel_size // 2 if padding == \"same\" else 0\n",
        "        self.conv = None\n",
        "        self.bn = None\n",
        "        self.relu = None\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okb3-6VwP5U0"
      },
      "source": [
        "### 1.2 - The stem block\n",
        "\n",
        "The stem block is designed as follows:\n",
        "\n",
        "<img src=\"https://docs.google.com/uc?export=download&id=1_QiuoSGysL4llyRjpeUhKzqf_qBlOMVj\" width=\"300px\">\n",
        "<caption><center> Stem block. </center></caption>\n",
        "\n",
        "Implement below all the steps by taking advantage of the above implemented ``conv2d_bn`` function for the blue rectangles. The values for the kernel sizes and the strides are specified in each block, use stride 1xs when it is not specified. Use padding \"valid\" when the letter *V* appears otherwise use padding \"same\". E.g., the first layer has the following parameters: 32 filters of shape (3, 3), stride of (2, 2), padding \"valid\" while the third one is composed of 64 filters of shape (3, 3), stride of (1, 1), padding \"same\".\n",
        "\n",
        "For the **Filter concat** layers (orange rectangles), you can use the torch.cat function and concatenate along the 'channel' axis (axis=1).\n",
        "\n",
        "**Note** that the last conv block has ``stride = 2`` and the max pooling layer has ``kernel = (3, 3)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X20h5o42PeKu"
      },
      "outputs": [],
      "source": [
        "class StemBlock(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.first_block = None\n",
        "        self.first_left = None\n",
        "        self.first_right = None\n",
        "        self.second_left =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.second_right =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.third_left = None\n",
        "        self.third_right = None\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = None\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi-mb3DOW48W"
      },
      "outputs": [],
      "source": [
        "model = StemBlock()\n",
        "print(model(torch.zeros((1, 3, 299, 299))).shape)\n",
        "print(f\"Expected shape (1, 384, 35, 35)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR87om4zC_ya"
      },
      "source": [
        "### 1.3 - The Inception-A block\n",
        "\n",
        "Implement below the Inception-A block as in the figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1wth4q2nf9h4d4JTwmrAhLVzb5pFyIRCd\" width=\"600px\">\n",
        "<caption><center> Inception-A block. </center></caption>\n",
        "\n",
        "**Note** The average pooling has ``pool_size = (3, 3)`` and ``strides = (1, 1)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ysZn3cXXCZ9"
      },
      "outputs": [],
      "source": [
        "class A_block(Module):\n",
        "\n",
        "    def __init__(self, in_filters):\n",
        "        super().__init__()\n",
        "        self.avg_block = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.one_by_one_block = None\n",
        "        self.three_by_three_block =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.five_by_five =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        x = None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T2FjEpEEc1h"
      },
      "outputs": [],
      "source": [
        "a_block = A_block(384)\n",
        "print(a_block(torch.zeros(1, 384, 35, 35)).shape)\n",
        "print(f\"Expected shape (1, 384, 35, 35)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQA7aB0ODPHJ"
      },
      "source": [
        "### 1.4 - The Inception-B block\n",
        "\n",
        "Implement below the Inception-B block as in the figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1j5W2jqmXblyDkpQJ6lSo77NCA0AjiFDN\" width=\"600px\">\n",
        "<caption><center> Inception-B block. </center></caption>\n",
        "\n",
        "**Note** The average pooling has ``pool_size = (3, 3)`` and ``strides = (1, 1)``. In the third branch, the last convolutional layer has kernel of size (7, 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVYLblJvErAg"
      },
      "outputs": [],
      "source": [
        "class B_block(Module):\n",
        "\n",
        "    def __init__(self, in_filters):\n",
        "        super().__init__()\n",
        "        self.avg_block = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.one_by_one_block = None\n",
        "\n",
        "        self.seven_by_seven_block =  Sequential(\n",
        "            None\n",
        "        )\n",
        "\n",
        "        self.thirteen_by_thirteen_block =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x=None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YlCM7A0FMbv"
      },
      "outputs": [],
      "source": [
        "b_block = B_block(1024)\n",
        "print(b_block(torch.zeros(1, 1024, 17, 17)).shape)\n",
        "print(f\"Expected shape (1, 1024, 17, 17)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5n5UBCSWsLG"
      },
      "source": [
        "### 1.5 - The Inception-C block\n",
        "\n",
        "Implement below the Inception-C block as in the figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1dzsLPAUI1YUHgU0WWwAyvIIlt2gX2IQH\" width=\"600px\">\n",
        "<caption><center> Inception-C block. </center></caption>\n",
        "\n",
        "**Note** The average pooling has ``pool_size = (3, 3)`` and ``strides = (1, 1)``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErSiYNqwFnrI"
      },
      "outputs": [],
      "source": [
        "class C_block(Module):\n",
        "\n",
        "    def __init__(self, in_filters):\n",
        "        super().__init__()\n",
        "        self.avg_block = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.one_by_one_block = None\n",
        "\n",
        "        self.branch_a = None\n",
        "        self.branch_a_left = None\n",
        "        self.branch_a_right = None\n",
        "\n",
        "        self.branch_b =  Sequential(\n",
        "            None\n",
        "        )\n",
        "\n",
        "        self.branch_b_left = None\n",
        "        self.branch_b_right = None\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF7ulLQ1Zfc_"
      },
      "outputs": [],
      "source": [
        "c_block = C_block(1536)\n",
        "print(c_block(torch.zeros(1, 1536, 8, 8)).shape)\n",
        "print(f\"Expected shape (1, 1536, 8, 8)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq58lB9qZDIv"
      },
      "source": [
        "### 1.6 - The Reduction-A block\n",
        "\n",
        "Implement below the Reduction-A block as in the figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1LtkLxcBOyJtVBikmY-ljspXhwCZlkHF1\" width=\"600px\">\n",
        "<caption><center> Reduction-A block. </center></caption>\n",
        "\n",
        "For the Inception-v4 the parameters are as follows:\n",
        "- n = 384\n",
        "- k = 192\n",
        "- l = 224\n",
        "- m = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK7PbRQEZRqP"
      },
      "outputs": [],
      "source": [
        "class Reduction_A(Module):\n",
        "\n",
        "    def __init__(self, in_filters):\n",
        "        super().__init__()\n",
        "        self.max_pool = None\n",
        "        self.central_block = None\n",
        "        self.right_block =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3SwIi4wa45f"
      },
      "outputs": [],
      "source": [
        "reduction_A = Reduction_A(384)\n",
        "print(reduction_A(torch.zeros(1, 384, 35, 35)).shape)\n",
        "print(\"Expected shape (1, 1024, 17, 17)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZBRm0GkbSm0"
      },
      "source": [
        "### 1.7 - The Reduction-B block\n",
        "\n",
        "Implement below the Reduction-B block as in the figure:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=18qKx5NE0i6U41EZnFhejYcTRnS_jlzp4\" width=\"600px\">\n",
        "\n",
        "<caption><center> Reduction-B block. </center></caption>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ognGkfChbSIn"
      },
      "outputs": [],
      "source": [
        "class Reduction_B(Module):\n",
        "\n",
        "    def __init__(self, in_filters):\n",
        "        super().__init__()\n",
        "        self.max_pool = None\n",
        "        self.central_block = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.right_block =  Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPrkQfLxcOF4"
      },
      "outputs": [],
      "source": [
        "reduction_B = Reduction_B(1024)\n",
        "print(reduction_B(torch.zeros(1, 1024, 17, 17)).shape)\n",
        "print(\"Expected shape (1, 1536, 8, 8)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkqMY8ohfF4z"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Dropout\n",
        "class InceptionV4(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = StemBlock()\n",
        "        self.inception_a = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.reduction_a = None\n",
        "        self.inception_b = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.reduction_b = None\n",
        "        self.inception_c = Sequential(\n",
        "            None\n",
        "        )\n",
        "        self.drop = None\n",
        "        self.out = None\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = None\n",
        "        return y\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eedUYiz7h49i"
      },
      "outputs": [],
      "source": [
        "inception_v4 = InceptionV4()\n",
        "print(inception_v4(torch.zeros(1, 3, 299, 299)).shape)\n",
        "print(\"Expected shape (1, 1)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehvt_MMaiI6K"
      },
      "outputs": [],
      "source": [
        "model = InceptionV4()\n",
        "opt = SGD(model.parameters(), lr=0.005)\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "epochs=10\n",
        "best_val = np.inf\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    iterator = tqdm(train_dataloader)\n",
        "    for batch_x, batch_y in iterator:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        y_pred = model(batch_x)\n",
        "\n",
        "        loss = loss_fn(y_pred, batch_y)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        iterator.set_description(f\"Train loss: {loss.detach().cpu().numpy()}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        true = []\n",
        "        for batch_x, batch_y in tqdm(valid_dataloader):\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            y_pred = model(batch_x)\n",
        "\n",
        "            predictions.append(y_pred)\n",
        "            true.append(batch_y)\n",
        "        predictions = torch.cat(predictions, axis=0)\n",
        "        true = torch.cat(true, axis=0)\n",
        "        val_loss = loss_fn(predictions, true)\n",
        "        val_acc = (torch.sigmoid(predictions).round() == true).float().mean()\n",
        "        print(f\"loss: {val_loss}, accuracy: {val_acc}\")\n",
        "    \n",
        "    if val_loss < best_val:\n",
        "        print(\"Saved Model\")\n",
        "        torch.save(model.state_dict(), \"model.pt\")\n",
        "        best_val = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYbRMTWi2tmS"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "evaluate_network(train_dataloader, model, \"Training Dataset\")\n",
        "evaluate_network(valid_dataloader, model, \"Validation Dataset\")\n",
        "evaluate_network(test_dataloader, model, \"Test Dataset\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iR87om4zC_ya",
        "oQA7aB0ODPHJ",
        "L5n5UBCSWsLG",
        "dq58lB9qZDIv",
        "NZBRm0GkbSm0"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}