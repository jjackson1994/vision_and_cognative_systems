{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRboflunHqye"
      },
      "source": [
        "# NEURAL NETWORKS AND DEEP LEARNING\n",
        "\n",
        "---\n",
        "A.A. 2022/23 (6 CFU) - Dr. Jacopo Pegoraro, Dr. Daniele Mari\n",
        "---\n",
        "\n",
        "\n",
        "## Lab. 03 - PyTorch regression and classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODcSwFcDXrtD"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBMlgnam0Yz0"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p82k5GjECklA"
      },
      "source": [
        "# Practical example: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QLy_wTqDVM1"
      },
      "source": [
        "Now let's solve the same problem analyzed in one of the previous lab using a simple neural network implemented in PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf9Zr5WjUZnB"
      },
      "source": [
        "# Data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0IGLgfUha0"
      },
      "source": [
        "Let's generate some data with our usual polynomial model, and save the data points in two csv files, one for training (train_data.csv), and one for validation (val_data.csv).\n",
        "\n",
        "You can find these files in the \"Files\" section of Colab. They are not stored in your local machine, but they are stored remotely in the Colab server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjGBsKhQUgwl"
      },
      "source": [
        "def poly_model(x, beta, noise_std=0):\n",
        "    \"\"\"\n",
        "    INPUT\n",
        "        x: x vector\n",
        "        beta: polynomial parameters\n",
        "        noise_std: enable noisy sampling (gaussian noise, zero mean, noise_std std)\n",
        "    \"\"\"\n",
        "    pol_order = len(beta)\n",
        "    x_matrix = np.array([x**i for i in range(pol_order)]).transpose()\n",
        "    y = np.matmul(x_matrix, beta)\n",
        "    noise = np.random.randn(len(y)) * noise_std\n",
        "    return y + noise\n",
        "\n",
        "beta_true = [-1.45, 1.12, 2.3]\n",
        "noise_std = 0.2\n",
        "np.random.seed(4)\n",
        "\n",
        "### Train data\n",
        "num_train_points = 20\n",
        "x_train = np.random.rand(num_train_points)\n",
        "y_train = poly_model(x_train, beta_true, noise_std)\n",
        "with open('train_data.csv', 'w') as f:\n",
        "    data = [f\"{x},{y}\" for x, y in zip(x_train, y_train)]\n",
        "    f.write('\\n'.join(data))\n",
        "    \n",
        "### Validation data\n",
        "num_val_points = 20\n",
        "x_val = np.random.rand(num_val_points)\n",
        "y_val = poly_model(x_val, beta_true, noise_std)\n",
        "with open('val_data.csv', 'w') as f:\n",
        "    data = [f\"{x},{y}\" for x, y in zip(x_val, y_val)]\n",
        "    f.write('\\n'.join(data))\n",
        "\n",
        "\n",
        "### Plot\n",
        "plt.figure(figsize=(12,8))\n",
        "x_highres = np.linspace(0,1,1000)\n",
        "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
        "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
        "plt.plot(x_val, y_val, color='g', ls='', marker='.', label='Validation data points')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg5hsr4Vk92E"
      },
      "source": [
        "# Network Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zusKyw-4EUN7"
      },
      "source": [
        "## Network Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV6KyWPAEYxL"
      },
      "source": [
        "Define a fully connected feed-forward network with 2 hidden layers.\n",
        "\n",
        "Use a sigmoid activation function.\n",
        "\n",
        "Since this is a regression, we do not want to limit the value of the output. For this reason, NO activation function should be used for the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCFiQ0t5EsnM"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self, Ni, Nh1, Nh2, No):\n",
        "        \"\"\"\n",
        "        Ni - Input size\n",
        "        Nh1 - Neurons in the 1st hidden layer\n",
        "        Nh2 - Neurons in the 2nd hidden layer\n",
        "        No - Output size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        print('Network initialized')\n",
        "        ###################################################\n",
        "        ### PUT YOR CODE HERE\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.out = None\n",
        "        self.act = None\n",
        "        ###################################################\n",
        "        \n",
        "    def forward(self, x, additional_out=False):\n",
        "        ###################################################\n",
        "        ### PUT YOR CODE HERE\n",
        "        x = None # don't forget the activations: not incuded by default!\n",
        "        ###################################################\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loocdtx_EZWw"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG-twDQsETb1"
      },
      "source": [
        "\n",
        "We have already implemented the dataset class in the previous lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilKnlHjBgG81"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lleqk78jfyBc"
      },
      "source": [
        "class CsvDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        # Read the file and split the lines in a list\n",
        "        with open(csv_file, 'r') as f:\n",
        "            lines = f.read().split('\\n')\n",
        "        # Get x and y values from each line and append to self.data\n",
        "        self.data = []\n",
        "        for line in lines:\n",
        "            sample = line.split(',')\n",
        "            self.data.append((float(sample[0]), float(sample[1])))\n",
        "        # Now self.data contains all our dataset.\n",
        "        # Each element of the list self.data is a tuple: (input, output)\n",
        "\n",
        "    def __len__(self):\n",
        "        # The length of the dataset is simply the length of the self.data list\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Our sample is the element idx of the list self.data\n",
        "        sample = self.data[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qYRE5c4gJoe"
      },
      "source": [
        "### Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcevK-ilf3iv"
      },
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        x, y = sample\n",
        "        return (torch.tensor([x]).float(),\n",
        "                torch.tensor([y]).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf8hVkF_D141"
      },
      "source": [
        "composed_transform = transforms.Compose([ToTensor()])\n",
        "\n",
        "train_dataset = CsvDataset('train_data.csv', transform=composed_transform)\n",
        "val_dataset = CsvDataset('val_data.csv', transform=composed_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nXQIlz3gPeP"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lvOg3jhFIe5"
      },
      "source": [
        "For the dataloader:\n",
        "\n",
        "* enable the shuffling only for training data\n",
        "* try different values for batch size\n",
        "* disable the multiprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Y1XPYQFQLk"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "val_dataloader  = DataLoader(val_dataset,  batch_size=len(val_dataset), shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWVcMe9jFCpv"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK8IMi5mGAxy"
      },
      "source": [
        "Now we put together all the steps analyzed in the previous lab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U24WThWLHcEC"
      },
      "source": [
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Training device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn7W-MAmgxai"
      },
      "source": [
        "# Initialize the network\n",
        "torch.manual_seed(0)\n",
        "Ni = 1\n",
        "Nh1 = 128\n",
        "Nh2 = 256\n",
        "No = 1\n",
        "net = Net(Ni, Nh1, Nh2, No)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDt5U_Wgg1Mh"
      },
      "source": [
        "# Define the loss function\n",
        "loss_fn = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgoFcUfPg116"
      },
      "source": [
        "# Define the optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLPzqHepg2RZ"
      },
      "source": [
        "num_epochs = 3000\n",
        "train_loss_log = []\n",
        "val_loss_log = []\n",
        "for epoch_num in range(num_epochs):\n",
        "    print('#################')\n",
        "    print(f'# EPOCH {epoch_num}')\n",
        "    print('#################')\n",
        "\n",
        "    ### TRAIN\n",
        "    train_loss= []\n",
        "    net.train() # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
        "    for sample_batched in train_dataloader:\n",
        "        # Move data to device\n",
        "        x_batch = sample_batched[0].to(device)\n",
        "        label_batch = sample_batched[1].to(device)\n",
        "\n",
        "        ###########################################\n",
        "        ### PUT YOUR CODE HERE\n",
        "        # Forward pass\n",
        "        out = None\n",
        "\n",
        "        # Compute loss\n",
        "        loss = None\n",
        "\n",
        "        # Backpropagation\n",
        "        None\n",
        "\n",
        "        # Update the weights\n",
        "        None\n",
        "        ###########################################\n",
        "\n",
        "        # Save train loss for this batch\n",
        "        loss_batch = loss.detach().cpu().numpy()\n",
        "        train_loss.append(loss_batch)\n",
        "\n",
        "    # Save average train loss\n",
        "    train_loss = np.mean(train_loss)\n",
        "    print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
        "    train_loss_log.append(train_loss)\n",
        "\n",
        "    ### VALIDATION\n",
        "    val_loss= []\n",
        "    net.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
        "    with torch.no_grad(): # Disable gradient tracking\n",
        "        for sample_batched in val_dataloader:\n",
        "            # Move data to device\n",
        "            x_batch = sample_batched[0].to(device)\n",
        "            label_batch = sample_batched[1].to(device)\n",
        "\n",
        "            ###########################################\n",
        "            ### PUT YOUR CODE HERE\n",
        "            # Forward pass\n",
        "            out = None\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "            ###########################################\n",
        "\n",
        "            # Save val loss for this batch\n",
        "            loss_batch = loss.detach().cpu().numpy()\n",
        "            val_loss.append(loss_batch)\n",
        "\n",
        "\n",
        "        # Save average validation loss\n",
        "        val_loss = np.mean(val_loss)\n",
        "        print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
        "        val_loss_log.append(val_loss)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS0i-CXukrPX"
      },
      "source": [
        "### Plot losses\n",
        "\n",
        "Additional tool for visualization: tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOY4yWW8lqwb"
      },
      "source": [
        "# Plot losses\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.semilogy(train_loss_log, label='Train loss')\n",
        "plt.semilogy(val_loss_log, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV0Kingak37m"
      },
      "source": [
        "# Network analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMe88uzesM2s"
      },
      "source": [
        "## Network output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW2JB_NqmhSo"
      },
      "source": [
        "# Input vector\n",
        "x_vec = torch.linspace(0,1,1000)\n",
        "x_vec = x_vec.to(device)\n",
        "x_vec = x_vec.unsqueeze(-1)  # Adding a dimension to the input vector\n",
        "print(f\"Input shape: {x_vec.shape}\")\n",
        "\n",
        "# Network output\n",
        "# eval() acts as switch for some specific layers/parts of the model that behave\n",
        "# differently during training and inference (eval) time. For example, Dropout \n",
        "# BatchNorm etc.  \n",
        "net.eval()\n",
        "with torch.no_grad(): # turn off gradients computation\n",
        "    y_vec = net(x_vec)\n",
        "print(f\"Output shape: {y_vec.shape}\")\n",
        "\n",
        "# Expected output\n",
        "beta_true = [-1.45, 1.12, 2.3]\n",
        "true_model = poly_model(x_vec.cpu().numpy(), beta_true).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ0atER6pl3S"
      },
      "source": [
        "# Convert x_vec and y_vec to numpy one dimensional arrays\n",
        "x_vec = x_vec.squeeze().cpu().numpy()\n",
        "y_vec = y_vec.squeeze().cpu().numpy()\n",
        "\n",
        "# Plot output\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x_vec, y_vec, label='Network output')\n",
        "plt.plot(x_vec, true_model, label='True model')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9SxwuMzsOQW"
      },
      "source": [
        "### What if we predict outside the range?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-kgTIEqsOQW"
      },
      "source": [
        "# Input vector\n",
        "x_vec = torch.linspace(-5,5,1000)\n",
        "x_vec = x_vec.to(device)\n",
        "x_vec = x_vec.unsqueeze(-1)  # Adding a dimension to the input vector\n",
        "print(f\"Input shape: {x_vec.shape}\")\n",
        "\n",
        "# Network output\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    y_vec = net(x_vec)\n",
        "print(f\"Output shape: {y_vec.shape}\")\n",
        "\n",
        "# Expected output\n",
        "beta_true = [-1.45, 1.12, 2.3]\n",
        "true_model = poly_model(x_vec.cpu().numpy(), beta_true).squeeze()\n",
        "\n",
        "# Convert x_vec and y_vec to numpy one dimensional arrays\n",
        "x_vec = x_vec.squeeze().cpu().numpy()\n",
        "y_vec = y_vec.squeeze().cpu().numpy()\n",
        "\n",
        "# Plot output\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x_vec, y_vec, label='Network output')\n",
        "plt.plot(x_vec, true_model, label='True model')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYrZyNsGsOQX"
      },
      "source": [
        "### What if we try different activation functions? (e.g., ReLU for nonlinearities)\n",
        "### Is there overfit with ReLU?\n",
        "\n",
        "### Homework : add some regularization to fight overfitting (e.g., dropout, weight decay,...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w7mO62wLQUG"
      },
      "source": [
        "## Access network parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDgv9tIJkvKn"
      },
      "source": [
        "# First hidden layer\n",
        "h1_w = net.fc1.weight.data.cpu().numpy()\n",
        "h1_b = net.fc1.bias.data.cpu().numpy()\n",
        "\n",
        "# Second hidden layer\n",
        "h2_w = net.fc2.weight.data.cpu().numpy()\n",
        "h2_b = net.fc2.bias.data.cpu().numpy()\n",
        "\n",
        "# Output layer\n",
        "out_w = net.out.weight.data.cpu().numpy()\n",
        "out_b = net.out.bias.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl66-WTllFlK"
      },
      "source": [
        "## Weights histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNS27jm0kSVH"
      },
      "source": [
        "# Weights histogram\n",
        "fig, axs = plt.subplots(3, 1, figsize=(12,8))\n",
        "axs[0].hist(h1_w.flatten(), 50)\n",
        "axs[0].set_title('First hidden layer weights')\n",
        "axs[1].hist(h2_w.flatten(), 50)\n",
        "axs[1].set_title('Second hidden layer weights')\n",
        "axs[2].hist(out_w.flatten(), 50)\n",
        "axs[2].set_title('Output layer weights')\n",
        "[ax.grid() for ax in axs]\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Xo32EvlJMt"
      },
      "source": [
        "#### Homework (simple): compare the histogram before and after the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG8FT5S5sOQY"
      },
      "source": [
        "## Save network parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3hQi0qMlRfI"
      },
      "source": [
        "### Save network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q45V3z1nkTQ5"
      },
      "source": [
        "### Save network parameters\n",
        "### Save the network state\n",
        "# The state dictionary includes all the parameters of the network\n",
        "net_state_dict = net.state_dict()\n",
        "print(net_state_dict.keys())\n",
        "# Save the state dict to a file\n",
        "torch.save(net_state_dict, 'net_parameters.torch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ez-ztylTWW"
      },
      "source": [
        "### Load network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u834VExlOe5"
      },
      "source": [
        "### Reload the network state\n",
        "# First initialize the network (if not already done)\n",
        "# IMPORTANT: you need to know the model definition!!\n",
        "net = Net(Ni, Nh1, Nh2, No) \n",
        "# Load the state dict previously saved\n",
        "net_state_dict = torch.load('net_parameters.torch')\n",
        "# Update the network parameters\n",
        "net.load_state_dict(net_state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlzmh2vZnJHz"
      },
      "source": [
        "## Save optimizer state\n",
        "Also the optimizer has its internal state!\n",
        "\n",
        "You need to save both the network and the optimizer states if you want to continue your training.\n",
        "\n",
        "If you are sure you have finished your training you can just save the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2kgoLEtnC2q"
      },
      "source": [
        "### Save the optimizer state\n",
        "torch.save(optimizer.state_dict(), 'optimizer_state.torch')\n",
        "\n",
        "### Reload the optimizer state\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "opt_state_dict = torch.load('optimizer_state.torch')\n",
        "optimizer.load_state_dict(opt_state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgeV0XEmm7yJ"
      },
      "source": [
        "## Analyze activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXdvC7e00z7_"
      },
      "source": [
        "# First naive way: simply change the network definition to return an additional output\n",
        "\n",
        "# More advanced strategy (optional if you're not familiar with Python!): using hooks\n",
        "\n",
        "def get_activation(layer, input, output):\n",
        "    global activation\n",
        "    activation = torch.sigmoid(output)\n",
        "\n",
        "### Register hook  \n",
        "hook_handle = net.fc2.register_forward_hook(get_activation)\n",
        "\n",
        "### Analyze activations\n",
        "net = net.to(device)\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    x1 = torch.tensor([0.1]).float().to(device)\n",
        "    y1 = net(x1)\n",
        "    z1 = activation\n",
        "    x2 = torch.tensor([0.9]).float().to(device)\n",
        "    y2 = net(x2)\n",
        "    z2 = activation\n",
        "    x3 = torch.tensor([2.5]).float().to(device)\n",
        "    y3 = net(x3)\n",
        "    z3 = activation\n",
        "\n",
        "### Remove hook\n",
        "hook_handle.remove()\n",
        "\n",
        "### Plot activations\n",
        "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
        "axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n",
        "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
        "axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n",
        "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
        "axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n",
        "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeBXPMglHLyP"
      },
      "source": [
        "# Exercise - Classification model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eo79cojLFvC"
      },
      "source": [
        "**HINTS**\n",
        "- Choose a loss function that is suitable for the specific problem, a binary classification in this case. If you keep a single linear output you can use a BCEWithLogitsLoss, which is more numerically stable than manually using a sigmoid output activation (more info here: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html ).\n",
        "- The network now has 2 inputs. A batched input should have a shape $\\text{batch_size} \\times 2$.\n",
        "- The dataset should be adapted accordingly. Also consider to increase the batch size.\n",
        "- Explore different optimizers, trying to understand the differences and their parameters (https://pytorch.org/docs/stable/optim.html ).\n",
        "- Try to increase the complexity of the network, and at the same time to introduce some regularization with dropout layers and/or weight decay (which is equivalent to an L2 regularization, typically implemented by the optimizer).\n",
        "- Experiment with different hyper-parameters trying to minimize the VALIDATION LOSS. Once you are happy with the result, try the final test with the TEST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNn5DYNZGNz4"
      },
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpICn2zOD4my"
      },
      "source": [
        "import itertools\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "def bidimensional_model(x1, x2):\n",
        "    a = 1\n",
        "    b = 1\n",
        "    cx1 = 0\n",
        "    cx2 = 0\n",
        "    out = (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 2\n",
        "    b = 2\n",
        "    cx1 = 5\n",
        "    cx2 = 5\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 1\n",
        "    b = 2\n",
        "    cx1 = -2.5\n",
        "    cx2 = 5\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 3\n",
        "    b = 1\n",
        "    cx1 = -6\n",
        "    cx2 = -2.5\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 2\n",
        "    b = 4\n",
        "    cx1 = -7.5\n",
        "    cx2 = -5\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 6\n",
        "    b = 1\n",
        "    cx1 = -7.5\n",
        "    cx2 = 5\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 4\n",
        "    b = 4\n",
        "    cx1 = 7.5\n",
        "    cx2 = -7.5\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 3\n",
        "    b = 2\n",
        "    cx1 = -1\n",
        "    cx2 = -6\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 2\n",
        "    b = 5\n",
        "    cx1 = 1\n",
        "    cx2 = 6\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    a = 2\n",
        "    b = 2\n",
        "    cx1 = 6\n",
        "    cx2 = 0\n",
        "    out |= (x1 - cx1)**2 / a**2 + (x2 - cx2)**2 / b **2 < 1\n",
        "    return out.astype(int)\n",
        "\n",
        "### PLOT MODEL\n",
        "# Input grid\n",
        "x1 = np.linspace(-10, 10, 400)\n",
        "x2 = np.linspace(-10, 10, 400)\n",
        "x_prod = [x for x in itertools.product(x1, x2)]\n",
        "x1 = np.array([x[0] for x in x_prod])\n",
        "x2 = np.array([x[1] for x in x_prod])\n",
        "# Evaluate out\n",
        "y = bidimensional_model(x1, x2)\n",
        "# Scatter plot\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "colors = np.array(['C0', 'C1'])\n",
        "ax.scatter(x1, x2, c=colors[y], s=10, marker='o')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYWWVtCyGT2k"
      },
      "source": [
        "### Training points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Jp4QJc95dq"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "### Train data\n",
        "num_points = 1000\n",
        "x1 = np.random.uniform(-10, 10, num_points)\n",
        "x2 = np.random.uniform(-10, 10, num_points)\n",
        "y = bidimensional_model(x1, x2)\n",
        "train_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
        "train_df.to_csv('classifier_train_data.csv', index=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(x1, x2, c=colors[y], s=10, marker='o')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_title('Training points')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SitIaovOGVvi"
      },
      "source": [
        "### Validation points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjRdHgXXFWic"
      },
      "source": [
        "In this case: we generate validation points.\n",
        "Usually: validation points are randomly selected from the training points (~20% of training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu1GcLMmJ7cS"
      },
      "source": [
        "### Validation data\n",
        "num_points = 200\n",
        "x1 = np.random.uniform(-10, 10, num_points)\n",
        "x2 = np.random.uniform(-10, 10, num_points)\n",
        "y = bidimensional_model(x1, x2)\n",
        "val_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
        "val_df.to_csv('classifier_val_data.csv', index=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(x1, x2, c=colors[y], s=10, marker='o')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_title('Validation points')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2jww94GXhM"
      },
      "source": [
        "### Test points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClhCIOQmKoIq"
      },
      "source": [
        "### Test data\n",
        "num_points = 400\n",
        "x1 = np.random.uniform(-10, 10, num_points)\n",
        "x2 = np.random.uniform(-10, 10, num_points)\n",
        "y = bidimensional_model(x1, x2)\n",
        "val_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
        "val_df.to_csv('classifier_test_data.csv', index=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(x1, x2, c=colors[y], s=10, marker='o')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_title('Test points')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAxFx__EGgWf"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiCbilBwHNaW"
      },
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUM50Gf4MCwZ"
      },
      "source": [
        "class ClassifierDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        ###############################\n",
        "        # PUT YOUR CODE HERE\n",
        "        self.transform = None\n",
        "        # Read the file and store the content in a pandas DataFrame\n",
        "        self.df = None\n",
        "        ###############################\n",
        "\n",
        "    def __len__(self):\n",
        "        ###############################\n",
        "        # PUT YOUR CODE HERE\n",
        "        # The length of the dataset is simply the length of the self.data list\n",
        "        return None\n",
        "        ###############################\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Our sample is the row at index idx of the dataframe\n",
        "\n",
        "        ###############################\n",
        "        # PUT YOUR CODE HERE\n",
        "        row = self.df.iloc[idx]\n",
        "        # There are 2 inputs this time\n",
        "        sample = ([row.x1, row.x2], row.y)\n",
        "        if self.transform:\n",
        "            sample = None\n",
        "        ###############################\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XUmOmF5HPxD"
      },
      "source": [
        "Define the transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBZbbBvHJq4"
      },
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        x, y = sample\n",
        "        return (torch.Tensor(x).float(),\n",
        "                torch.Tensor([y]).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NR9xVDbnQRk"
      },
      "source": [
        "Initialize the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg443m60HMAZ"
      },
      "source": [
        "###############################\n",
        "### PUT YOUR CODE HERE\n",
        "composed_transform = transforms.Compose([ToTensor()])\n",
        "\n",
        "train_dataset = None\n",
        "val_dataset   = None\n",
        "test_dataset  = None\n",
        "###############################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43MlfiTOHTL0"
      },
      "source": [
        "Define the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5n2vVDgJfnM"
      },
      "source": [
        "###############################\n",
        "#Put your code here\n",
        "train_dataloader = None #batch size = 200\n",
        "val_dataloader   = None #The batch size can be equal to the validation set size if it fits in memory\n",
        "test_dataloader  = None\n",
        "###############################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4rTz2L4G6Rf"
      },
      "source": [
        "## Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxrTC-LPG8XJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self, Ni, Nh1, Nh2, No):\n",
        "        \"\"\"\n",
        "        Ni - Input size\n",
        "        Nh1 - Neurons in the 1st hidden layer\n",
        "        Nh2 - Neurons in the 2nd hidden layer\n",
        "        No - Output size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        print('Network initialized')\n",
        "        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh1)\n",
        "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
        "        self.out = nn.Linear(in_features=Nh2, out_features=No)\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, additional_out=False):\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yha8MKgSG1f-"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1AKIP44G3JR"
      },
      "source": [
        "# Initialize the network\n",
        "torch.manual_seed(0)\n",
        "Ni = 2\n",
        "Nh1 = 128\n",
        "Nh2 = 256\n",
        "No = 1 #now we have classification between two classes\n",
        "\n",
        "net = Net(Ni, Nh1, Nh2, No)\n",
        "net.to(device)\n",
        "\n",
        "# Define the loss function\n",
        "###############################################\n",
        "###PUT YOUR CODE HERE\n",
        "loss_fn = None # TODO: use a proper classification loss\n",
        "\n",
        "# Define the optimizer with lr=1e-2\n",
        "optimizer = None\n",
        "###############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmgre8qHKP8R"
      },
      "source": [
        "### TRAINING LOOP\n",
        "num_epochs = 600\n",
        "train_loss_log = []\n",
        "val_loss_log = []\n",
        "for epoch_num in range(num_epochs):\n",
        "    print('#################')\n",
        "    print(f'# EPOCH {epoch_num}')\n",
        "    print('#################')\n",
        "\n",
        "    ### TRAIN\n",
        "    train_loss= []\n",
        "    ###############################################\n",
        "    ###PUT YOUR CODE HERE\n",
        "    net.train() # Training mode (e.g. enable dropout)\n",
        "    for sample_batched in train_dataloader:\n",
        "        # Move data to device\n",
        "        x_batch = None\n",
        "        label_batch = None\n",
        "\n",
        "        # Forward pass\n",
        "        out = None\n",
        "\n",
        "        # Compute loss\n",
        "        loss = None\n",
        "\n",
        "        # Backpropagation\n",
        "        None\n",
        "\n",
        "        # Update the weights\n",
        "        None\n",
        "\n",
        "        # Save train loss for this batch\n",
        "        loss_batch = loss.detach().cpu().numpy()\n",
        "        train_loss.append(loss_batch)\n",
        "\n",
        "    # Save average train loss\n",
        "    train_loss = np.mean(train_loss)\n",
        "    print(f\"AVERAGE TRAIN LOSS: {train_loss}\")\n",
        "    train_loss_log.append(train_loss)\n",
        "\n",
        "    ### VALIDATION\n",
        "    val_loss= []\n",
        "    net.eval() # Evaluation mode (e.g. disable dropout)\n",
        "    with torch.no_grad(): # Disable gradient tracking\n",
        "        for sample_batched in val_dataloader:\n",
        "            # Move data to device\n",
        "            x_batch = None\n",
        "            label_batch = None\n",
        "\n",
        "            # Forward pass\n",
        "            out=None\n",
        "\n",
        "            # Compute loss\n",
        "            loss = None\n",
        "\n",
        "            # Save val loss for this batch\n",
        "            loss_batch = loss.detach().cpu().numpy()\n",
        "            val_loss.append(loss_batch)\n",
        "        ###############################################\n",
        "        # Save average validation loss\n",
        "        val_loss = np.mean(val_loss)\n",
        "        print(f\"AVERAGE VAL LOSS: {np.mean(val_loss)}\")\n",
        "        val_loss_log.append(val_loss)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZISd_5XK5HS"
      },
      "source": [
        "# Plot losses\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.semilogy(train_loss_log, label='Train loss')\n",
        "plt.semilogy(val_loss_log, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuSQe7WCP_Cz"
      },
      "source": [
        "## Final test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BkLq7wDnurL"
      },
      "source": [
        "Iterate the dataloader a single time and save all the outputs (in case you have multiple batches)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bksmig_P768"
      },
      "source": [
        "all_inputs = []\n",
        "all_outputs = []\n",
        "all_labels = []\n",
        "net.eval() # Evaluation mode (e.g. disable dropout)\n",
        "with torch.no_grad(): # Disable gradient tracking\n",
        "    for sample_batched in test_dataloader:\n",
        "        # Move data to device\n",
        "        x_batch = sample_batched[0].to(device)\n",
        "        label_batch = sample_batched[1].to(device)\n",
        "        # Forward pass\n",
        "        out = net(x_batch)\n",
        "        # Save outputs and labels\n",
        "        all_inputs.append(x_batch)\n",
        "        all_outputs.append(out)\n",
        "        all_labels.append(label_batch)\n",
        "# Concatenate all the outputs and labels in a single tensor\n",
        "all_inputs  = torch.cat(all_inputs)\n",
        "all_outputs = torch.cat(all_outputs)\n",
        "all_labels  = torch.cat(all_labels)\n",
        "\n",
        "test_loss = loss_fn(all_outputs, all_labels)\n",
        "print(f\"AVERAGE TEST LOSS: {test_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC-ZPwuqUZEh"
      },
      "source": [
        "In this case the network has a linear output (for a better stability of the loss function). \n",
        "To have probability estimates you can apply a sigmoid to the network output.\n",
        "\n",
        "Since we just need the most probable class and we have a single output, we can consider the sign of the linear output. Negative output means that the class 0 is the most probable (probability < 50%), otherwise class 1 (probability > 50%).\n",
        "\n",
        "Essentially this network estimates the probability of the input sample to be of class 1.\n",
        "\n",
        "> **NOTE**\n",
        "> \n",
        "> You can (and should, for practice) redefine the problem by defining a network with more than one output, each of them corresponding to a specific class (2 in this case). Since the two classes are mutually exclusive, the loss function should be a CrossEntropyLoss (softmax activation). In a multi-class scenario, a BCE loss is suitable when the classes are NOT mutually exclusive.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcAovj6bnpHy"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Get the most probable class inferred by the network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4-s4wdVSmrR"
      },
      "source": [
        "# Get the most probable class inferred by the network\n",
        "all_output_classes = torch.zeros(all_outputs.shape).to(device)\n",
        "all_output_classes[all_outputs > 0] = # TODO: what's going on here?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MKpD6NCnX4E"
      },
      "source": [
        "Evaluate the test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNk9_PjUSB2y"
      },
      "source": [
        "tot_correct_out = (all_output_classes == all_labels).sum()\n",
        "test_accuracy = 100 * tot_correct_out / len(all_labels)\n",
        "print(f\"TEST ACCURACY: {test_accuracy:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55ciPBknq9P"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pI5we58asHV"
      },
      "source": [
        "### Plot\n",
        "x1 = all_inputs.squeeze().cpu().numpy()[:, 0]\n",
        "x2 = all_inputs.squeeze().cpu().numpy()[:, 1]\n",
        "y_true = all_labels.squeeze().cpu().numpy()\n",
        "y_pred = all_output_classes.squeeze().cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "# Plot predictions\n",
        "ax.scatter(x1, x2, c=colors[y_pred.astype(np.uint8)], s=10, marker='o')\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_title('Network Predictions')\n",
        "# Mark wrong outputs\n",
        "error_mask = y_pred != y_true\n",
        "ax.scatter(x1[error_mask], x2[error_mask], color='red', s=100, marker='x', label='MISCLASSIFIED SAMPLES')\n",
        "plt.legend()\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MJqzdQsOQg"
      },
      "source": [
        "# (Optional) High-level and Lightweight PyTorch wrapper: PyTorch Lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfHIhPKisOQh"
      },
      "source": [
        "Lightning forces the following structure to your code which makes it reusable and shareable:\n",
        "\n",
        "Research code (the LightningModule).\n",
        "Engineering code (you delete, and is handled by the Trainer).\n",
        "Non-essential research code (logging, etc... this goes in Callbacks).\n",
        "Data (use PyTorch DataLoaders or organize them into a LightningDataModule).\n",
        "Once you do this, you can train on multiple-GPUs, TPUs, CPUs and even in 16-bit precision without changing your code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPcpIEpIsOQh"
      },
      "source": [
        "! pip install pytorch-lightning\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPq03tUzsOQh"
      },
      "source": [
        "# Define a LightningModule (nn.Module subclass)\n",
        "# A LightningModule defines a full system (ie: a GAN, autoencoder, BERT or a simple Image Classifier).\n",
        "class LitNet(pl.LightningModule):\n",
        "    def __init__(self, Ni, Nh1, Nh2, No):\n",
        "        \"\"\"\n",
        "        Ni - Input size\n",
        "        Nh1 - Neurons in the 1st hidden layer\n",
        "        Nh2 - Neurons in the 2nd hidden layer\n",
        "        No - Output size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        print('Network initialized')\n",
        "        self.net = nn.Sequential(nn.Linear(in_features=Ni, out_features=Nh1), \n",
        "                       nn.Sigmoid(), \n",
        "                       nn.Linear(in_features=Nh1, out_features=Nh2),\n",
        "                       nn.Sigmoid(),\n",
        "                       nn.Linear(in_features=Nh2, out_features=No))\n",
        "        self.val_loss = []\n",
        "        self.train_loss = []\n",
        "\n",
        "    # Forward step defines how the LightningModule behaves during inference/prediction.\n",
        "    def forward(self, x, additional_out=False):\n",
        "        return self.net(x)\n",
        "\n",
        "    # Training_step defines the training loop. \n",
        "    def training_step(self, batch, batch_idx=None):\n",
        "        # training_step defines the train loop. It is independent of forward\n",
        "        x_batch = batch[0]\n",
        "        label_batch = batch[1]\n",
        "        out = self.net(x_batch)\n",
        "        loss = F.binary_cross_entropy_with_logits(out, label_batch)\n",
        "        self.train_loss.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx=None):\n",
        "        # validation_step defines the validation loop. It is independent of forward\n",
        "        x_batch = batch[0]\n",
        "        label_batch = batch[1]\n",
        "        out = self.net(x_batch)\n",
        "        loss = F.binary_cross_entropy_with_logits(out, label_batch)\n",
        "        self.val_loss.append(loss.item())\n",
        "        self.log(\"val_loss\", loss.item(), prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.net.parameters(), lr=1e-2)\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ideebe2sOQh"
      },
      "source": [
        "# TRAIN!\n",
        "batch_size = 200\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_dataloader   = DataLoader(val_dataset,   batch_size=len(val_dataset), shuffle=False, num_workers=0)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size=len(test_dataset), shuffle=False, num_workers=0)\n",
        "\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=20, val_check_interval=1)\n",
        "litnet = LitNet(Ni, Nh1, Nh2, No)\n",
        "trainer.fit(litnet, train_dataloader, val_dataloader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAve8uC2jAbL"
      },
      "source": [
        "# Plot losses\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.semilogy(litnet.train_loss, label='Train loss')\n",
        "plt.semilogy(litnet.val_loss, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YxnHwevsOQh"
      },
      "source": [
        "# (Optional) Automatic Hyper-parameters Tuning: Optuna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuEU_L3jsOQi"
      },
      "source": [
        "Optuna is an automatic hyperparameter optimization software framework.\n",
        "\n",
        "- Lightweight, versatile, and platform agnostic architecture\n",
        "- Pythonic search spaces\n",
        "- Efficient optimization algorithms\n",
        "- Easy parallelization\n",
        "- Quick visualization\n",
        "\n",
        "\n",
        "Definitions:\n",
        "\n",
        "- Study: optimization based on an objective function\n",
        "- Trial: a single execution of the objective function\n",
        "\n",
        "The goal of a study is to find out the optimal set of hyperparameter values (e.g., classifier and svm_c) through multiple trials (e.g., n_trials=100). Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n",
        "\n",
        "Optuna works with PyTorch, but also with Tensorflow, Keras and PyTorch Lightning! (and many others)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cczfouc7rrCn"
      },
      "source": [
        "! pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix7PMDrmsOQi"
      },
      "source": [
        "# Before we used: \n",
        "#Nh1 = 128\n",
        "#Nh2 = 256\n",
        "\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # We optimize the number of hidden units in each layer.\n",
        "    output_dims = [\n",
        "        trial.suggest_int(\"n_units_l{}\".format(i), 64, 256, log=True) for i in range(2)\n",
        "    ]\n",
        "\n",
        "    model = LitNet(Ni, output_dims[0], output_dims[1], No)\n",
        "\n",
        "    trainer = pl.Trainer(gpus=1, max_epochs=20, val_check_interval=1, \n",
        "                         log_every_n_steps=1)\n",
        "    hyperparameters = dict(output_dims=output_dims)\n",
        "    trainer.logger.log_hyperparams(hyperparameters)\n",
        "    trainer.fit(model, train_dataloader, val_dataloader)\n",
        "    return trainer.callback_metrics[\"val_loss\"].item()\n",
        "\n",
        "\n",
        "pruner = optuna.pruners.NopPruner()\n",
        "# print(pruner) <optuna.pruners._nop.NopPruner object at 0x7f4c2466ed50>\n",
        "# print(type(pruner)) <class 'optuna.pruners._nop.NopPruner'>\n",
        "\n",
        "study = optuna.create_study(study_name=\"myfirstoptimizationstudy\", direction=\"minimize\", pruner=pruner)\n",
        "study.optimize(objective, n_trials=3, timeout=300)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ZKPakhuFSi"
      },
      "source": [
        "# Homework\n",
        "\n",
        "Try tuning other hyper-parameters, such as the learning rate."
      ]
    }
  ]
}